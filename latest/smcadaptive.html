<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adaptive resampling · SequentialMonteCarlo.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SequentialMonteCarlo.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Contents</a></li><li><a class="toctext" href="intro.html">Introduction</a></li><li><a class="toctext" href="smcintegrals.html">SMC integrals</a></li><li><a class="toctext" href="smcalgorithm.html">SMC algorithm</a></li><li><a class="toctext" href="smctheory.html">Theoretical properties</a></li><li><a class="toctext" href="smcve.html">Variance estimators</a></li><li class="current"><a class="toctext" href="smcadaptive.html">Adaptive resampling</a><ul class="internal"><li><a class="toctext" href="#(Relative)-effective-sample-size-1">(Relative) effective sample size</a></li><li><a class="toctext" href="#Adaptive-resampling-algorithm-1">Adaptive resampling algorithm</a></li><li><a class="toctext" href="#Particle-approximations-1">Particle approximations</a></li><li><a class="toctext" href="#Variance-estimation-1">Variance estimation</a></li></ul></li><li><a class="toctext" href="csmc.html">Conditional SMC</a></li><li><a class="toctext" href="impl.html">Implementation notes</a></li><li><a class="toctext" href="smcinterface.html">SMC interface</a></li><li><a class="toctext" href="guide.html">Types and functions</a></li><li><a class="toctext" href="hmm.html">Hidden Markov models</a></li><li><a class="toctext" href="refs.html">References</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="smcadaptive.html">Adaptive resampling</a></li></ul><a class="edit-page" href="https://github.com/awllee/SequentialMonteCarlo.jl/blob/master/docs/src/smcadaptive.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Adaptive resampling</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="adaptiveresampling-1" href="#adaptiveresampling-1">SMC with adaptive resampling</a></h1><p>There is a tuning parameter associated with the <code>SequentialMonteCarlo.smc</code> algorithm that results in an adaptive version of the <a href="smcalgorithm.html#basicsmc-1">SMC Algorithm</a>. This is the <code>essThreshold</code> parameter, whose use was proposed by Kong et al. (1994) and Liu &amp; Chen (1995). We represent this parameter as <span>$\tau$</span> below.</p><h2><a class="nav-anchor" id="(Relative)-effective-sample-size-1" href="#(Relative)-effective-sample-size-1">(Relative) effective sample size</a></h2><p>The adaptive SMC algorithm essentially involves choosing <span>$(A_{p-1}^{1}, \ldots, A_{p-1}^N) = \left(1, \ldots, N \right)$</span> when the <em>weights</em> associated with particles <span>$\zeta_{p-1}^{1}, \ldots, \zeta_{p-1}^N$</span> have a <em>relative effective sample size</em> exceeding <span>$\tau$</span>. This is sometimes referred to as &quot;not resampling&quot;. The relative effective sample size of a collection of weights is defined as \[ {\rm rESS}(w_{1}, \ldots, w_{N}) := \frac{\left(\frac{1}{N}\sum_{i=1}^N w_{i} \right)^{2}}{\frac{1}{N} \sum_{i=1}^N w_{i}^{2}}. \] This function is invariant to rescaling of all of the weights by a common constant.</p><p>In the <a href="smcalgorithm.html#basicsmc-1">SMC Algorithm</a> we can view the weights associated with the particles <span>$\zeta_{p-1}^{1}, \ldots, \zeta_{p-1}^N$</span> as being <span>$W_{p-1}^{1}, \ldots, W_{p-1}^N$</span> where <span>$W_{p-1}^{i} \propto G_{p-1}(\zeta_{p-1}^{i})$</span>, and their relative effective sample size is <span>${\rm rESS}(W_{p-1}^{1}, \ldots, W_{p-1}^N)$</span>.</p><p>In the SMC with adaptive resampling algorithm, the weight of a particle may be proportional to a product of many potential function values, depending on when the most recent normal resampling step was. In particular, whenever the standard resampling step is not taken, particles first inherit the weights of their ancestors and then multiply them by their own potential function values. The algorithm is as follows:</p><h2><a class="nav-anchor" id="Adaptive-resampling-algorithm-1" href="#Adaptive-resampling-algorithm-1">Adaptive resampling algorithm</a></h2><ol><li><p>Sample <span>$\zeta_{1}^{i} \overset{\mathrm{i.i.d.}}{\sim} M_{1}$</span> and compute <span>$W_{1}^{i} \propto G_{1}(\zeta_{1}^{i})$</span> for <span>$i\in \{1, \ldots, N\}$</span>.</p></li><li><p>For <span>$p=2,\ldots,n$</span>:</p><ul><li><p>compute <span>$\mathcal{E}_{p-1}^N := {\rm rESS}(W_{p-1}^{1},\ldots,W_{p-1}^N)$</span>.</p></li><li><p>if <span>$p = n$</span> or <span>$\mathcal{E}_{p-1}^N \leq \tau$</span> set <span>$R_{p-1} \leftarrow 1$</span>; otherwise set <span>$R_{p-1} \leftarrow 0$</span>.</p></li><li><p>if <span>$R_{p-1} = 1$</span>, sample a vector <span>$A_{p-1}^{1}, \ldots, A_{p-1}^N$</span> of i.i.d. <span>${\rm Categorical}(W_{p-1}^{1}, \ldots, W_{p-1}^N)$</span> random variables in increasing order; otherwise set <span>$(A_{p-1}^{1}, \ldots, A_{p-1}^N) = \left(1, \ldots, N \right)$</span>.</p></li><li><p>sample <span>$\zeta_{p}^{i} \overset{\mathrm{ind}}{\sim} M_{p}(\zeta_{p-1}^{A_{p-1}^{i}}, \cdot)$</span> and compute <span>$W_{p}^{i} \propto \left(W_{p-1}^{i} \right)^{\mathbb{I}(R_{p-1} = 0)} G_{p}(\zeta_{p}^{i})$</span> for <span>$i\in \{1,\ldots,N\}$</span>.</p></li></ul></li></ol><hr/><p>Note that the time <span>$n-1$</span> particles are always resampled, so that <span>$\eta_{n}^N$</span> is always an unweighted particle approximation of <span>$\eta_{n}$</span>. This is primarily an implementation detail.</p><h2><a class="nav-anchor" id="Particle-approximations-1" href="#Particle-approximations-1">Particle approximations</a></h2><p>We define \[ \eta_{p}^N \propto \sum_{i=1}^N \left(W_{p-1}^{i} \right)^{ \mathbb{I}(R_{p-1} = 0)} \delta_{\zeta_{p}^{i}}, \qquad p \in \{1,\ldots,n\}, \] and \[ \hat{\eta}_{p}^N \propto \sum_{i=1}^N W_{p}^{i} \delta_{\zeta_{p}^{i}}, \qquad p \in \{1,\ldots,n\}. \]</p><p>Appropriate approximations of <span>$\hat{Z}_{1}^N, \ldots, \hat{Z}_{n}^N$</span> are also well-defined, but tedious to display.</p><p>These particle approximations all enjoy the same <a href="smctheory.html#maintheory-1">theoretical properties</a> stated for the standard SMC Algorithm, although the asymptotic variance maps <span>$\sigma^2_p$</span> are generally different. In fact, adaptive resampling improves in certain scenarios the quality of SMC approximations; its effects have been studied theoretically by Del Moral et al. (2010) and Whiteley et al. (2016).</p><h2><a class="nav-anchor" id="Variance-estimation-1" href="#Variance-estimation-1">Variance estimation</a></h2><p>All of the methods described in <a href="smcinterface.html#Variance-estimators-1">Variance estimators</a> can be run on output from the SMC with adaptive resampling algorithm.</p><p>One should be aware that the length of the vector returned by <code>SequentialMonteCarlo.vpns</code> will be of length <span>$m = 1 + \sum_{i=1}^{n-1} R_i \leq n$</span>. This is a consequence of the fact that resampling only at certain times can be viewed as running an SMC algorithm with modified Markov kernels and potential functions defined on a more complicated state space –- the details are not presented here –- so that the number of terms in the asymptotic variance decomposition is <span>$m$</span> and not (necessarily) <span>$n$</span>.</p><footer><hr/><a class="previous" href="smcve.html"><span class="direction">Previous</span><span class="title">Variance estimators</span></a><a class="next" href="csmc.html"><span class="direction">Next</span><span class="title">Conditional SMC</span></a></footer></article></body></html>
