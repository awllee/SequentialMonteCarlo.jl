<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Variance estimators · SequentialMonteCarlo.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SequentialMonteCarlo.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Contents</a></li><li><a class="toctext" href="intro.html">Introduction</a></li><li><a class="toctext" href="smcintegrals.html">SMC integrals</a></li><li><a class="toctext" href="smcalgorithm.html">SMC algorithm</a></li><li><a class="toctext" href="smctheory.html">Theoretical properties</a></li><li class="current"><a class="toctext" href="smcve.html">Variance estimators</a><ul class="internal"><li><a class="toctext" href="#Asymptotic-variances-1">Asymptotic variances</a></li><li><a class="toctext" href="#The-estimators-1">The estimators</a></li><li><a class="toctext" href="#Theoretical-justification-1">Theoretical justification</a></li></ul></li><li><a class="toctext" href="smcadaptive.html">Adaptive resampling</a></li><li><a class="toctext" href="csmc.html">Conditional SMC</a></li><li><a class="toctext" href="impl.html">Implementation notes</a></li><li><a class="toctext" href="performance.html">Performance tips</a></li><li><a class="toctext" href="smcinterface.html">SMC interface</a></li><li><a class="toctext" href="guide.html">Types and functions</a></li><li><a class="toctext" href="hmm.html">Hidden Markov models</a></li><li><a class="toctext" href="bench.html">Benchmarks</a></li><li><a class="toctext" href="refs.html">References</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="smcve.html">Variance estimators</a></li></ul><a class="edit-page" href="https://github.com/awllee/SequentialMonteCarlo.jl/blob/master/docs/src/smcve.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Variance estimators</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="vepage-1" href="#vepage-1">Variance estimators</a></h1><h2><a class="nav-anchor" id="Asymptotic-variances-1" href="#Asymptotic-variances-1">Asymptotic variances</a></h2><p>We recall that the particle approximations have <a href="smctheory.html#maintheory-1">asymptotic variances</a> associated with particular test functions <span>$f$</span>. Letting <span>$n$</span> be arbitrary to avoid additional indexing notation, the values <span>$\sigma_n^2(f)$</span> and <span>$\hat{\sigma}_n^2(f)$</span> are typically not possible to calculate exactly. They can be decomposed into sums as follows: \[ \sigma_n^2(f) = \sum_{p=1}^n v_{p,n}(f), \qquad \hat{\sigma}_n^2(f) = \sum_{p=1}^n \hat{v}_{p,n}(f).\] The quantities <span>$v_{p,n}(f)$</span> and <span>$\hat{v}_{p,n}(f)$</span> also cannot be computed exactly in general, but this decomposition can in some circumstances shed some light on the nature of the approximation errors.</p><h2><a class="nav-anchor" id="The-estimators-1" href="#The-estimators-1">The estimators</a></h2><p>Most Monte Carlo approximations are accompanied by Monte Carlo approximations of their asymptotic variance or MSE. Lee and Whiteley (2015) defined <span>$V_p^N(f)$</span>, computable using the simulated particle system, allowing one to approximate asymptotic variances of interest.</p><table><tr><th>approximation</th><th>approximation of</th></tr><tr><td><span>$V_p^N(f)$</span></td><td><span>${\rm var} \left \{ \gamma_p^N(f)/\gamma_p(1) \right \}$</span></td></tr><tr><td><span>$V_p^N(f-\eta_p^N(f))$</span></td><td><span>$\mathbb{E} \left[ \left\{ \eta_p^N(f) - \eta_p(f) \right\}^2 \right]$</span></td></tr><tr><td><span>$\hat{V}_p^N(f)$</span></td><td><span>${\rm var} \left\{ \hat{\gamma}_p^N(f) / \hat{\gamma}_p(1) \right\}$</span></td></tr><tr><td><span>$\hat{V}_p^N(f-\hat{\eta}_p^N(f))$</span></td><td><span>$\mathbb{E} \left[ \left\{ \hat{\eta}_p^N(f) - \hat{\eta}_p(f) \right\} ^2\right]$</span></td></tr></table><p>These approximations have the following theoretical justification, particularly for their approximation of the asymptotic variance and MSE maps defined earlier. Again, we present the results only for the &quot;updated&quot; quantities to avoid repetition; analogous results hold for their &quot;unhatted&quot; counterparts.</p><h2><a class="nav-anchor" id="Theoretical-justification-1" href="#Theoretical-justification-1">Theoretical justification</a></h2><p>The variance approximations have consistency properties, and some of them have lack-of-bias properties.</p><hr/><p><strong>Theorem</strong> [Lee &amp; Whiteley, 2015]. Let the potential functions <span>$G_1, \ldots, G_n$</span> be bounded and strictly positive. The following hold for an arbitrary, bounded <span>$f$</span>:</p><ol><li><p>Lack-of-bias: <span>$\mathbb{E}\left[(\hat{Z}_p^N)^2\hat{V}_p^N(f)\right]={\rm var}\left\{ \hat{\gamma}_p(f)\right\}$</span> for all <span>$N \geq 1$</span>.</p></li><li><p>Consistency: <span>$N\hat{V}_p^N(f) \overset{P}{\rightarrow} \hat{\sigma}_p^2(f)$</span> and <span>$N \hat{V}_p^N(f-\hat{\eta}_p^N(f)) \overset{P}{\rightarrow} \hat{\sigma}_p^2(f - \hat{\eta}_p(f))$</span>.</p></li></ol><hr/><p><strong>Remark</strong>. A consistent estimator of the asymptotic MSE of <span>$\eta_p(f)$</span> or <span>$\hat{\eta}_p(f)$</span> was proposed in Chan &amp; Lai (2013). This is very similar to that of Lee &amp; Whiteley (2015) but does not satisfy the lack-of-bias property.</p><p>Lee &amp; Whiteley (2015) also define approximations <span>$v_{p,n}^N(f)$</span> and <span>$\hat{v}_{p,n}^N(f)$</span> of <span>$v_{p,n}(f)$</span> and <span>$\hat{v}_{p,n}(f)$</span>, respectively. Their sums <span>$v_n^N(f)=\sum_{p=1}^nv_{p,n}^N(f)$</span> and <span>$\hat{v}_n^N(f)=\sum_{p=1}^n\hat{v}_{p,n}^N(f)$</span> can also be used as alternative approximations of <span>$\sigma_n^2(f)$</span> and <span>$\hat{\sigma}_n^2(f)$</span></p><hr/><p><strong>Theorem</strong> [Lee &amp; Whiteley, 2015]. Let the potential functions <span>$G_1, \ldots, G_n$</span> be bounded and strictly positive. The following hold for an arbitrary, bounded <span>$f$</span>:</p><ol><li><p>Lack-of-bias: <span>$\mathbb{E}\left[\left(\hat{Z}_n^N\right)^2\hat{v}_{p,n}^N(f)\right]=\hat{Z}_n^2\hat{v}_{p,n}(f)$</span> for all <span>$N\geq1$</span>.</p></li><li><p>Consistency: <span>$\hat{v}_{p,n}^N(f)\overset{P}{\rightarrow}\hat{v}_{p,n}(f)$</span> and <span>$\hat{v}_{p,n}^N(f)(f-\hat{\eta}_n^N(f))\overset{P}{\rightarrow}\hat{v}_{p,n}(f-\hat{\eta}_n(f))$</span>.</p></li><li><p>Lack-of-bias: <span>$\mathbb{E}\left[\left(\hat{Z}_n^N\right)^2\hat{v}_n^N(f)\right]=\hat{Z}_n^2\hat{\sigma}_n^2(f)$</span> for all <span>$N\geq1$</span>.</p></li><li><p>Consistency: <span>$\hat{v}_n^N(f)\overset{P}{\rightarrow}\hat{\sigma}_n^2(f)$</span> and <span>$\hat{v}_n^N(f-\hat{\eta}_n^N(f))\overset{P}{\rightarrow}\hat{\sigma}_n^2(f-\hat{\eta}_n(f))$</span>.</p></li></ol><hr/><footer><hr/><a class="previous" href="smctheory.html"><span class="direction">Previous</span><span class="title">Theoretical properties</span></a><a class="next" href="smcadaptive.html"><span class="direction">Next</span><span class="title">Adaptive resampling</span></a></footer></article></body></html>
