var documenterSearchIndex = {"docs":
[{"location":"smcadaptive/#adaptiveresampling-1","page":"Adaptive resampling","title":"SMC with adaptive resampling","text":"","category":"section"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"There is a tuning parameter associated with the SequentialMonteCarlo.smc algorithm that results in an adaptive version of the SMC Algorithm. This is the essThreshold parameter, whose use was proposed by Kong et al. (1994) and Liu & Chen (1995). We represent this parameter as tau below.","category":"page"},{"location":"smcadaptive/#(Relative)-effective-sample-size-1","page":"Adaptive resampling","title":"(Relative) effective sample size","text":"","category":"section"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"The adaptive SMC algorithm essentially involves choosing (A_p-1^1 ldots A_p-1^N) = left(1 ldots N right) when the weights associated with particles zeta_p-1^1 ldots zeta_p-1^N have a relative effective sample size exceeding tau. This is sometimes referred to as \"not resampling\". The relative effective sample size of a collection of weights is defined as","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"rm rESS(w_1 ldots w_N) = fracleft(frac1Nsum_i=1^N w_i right)^2frac1N sum_i=1^N w_i^2","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"This function is invariant to rescaling of all of the weights by a common constant.","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"In the SMC Algorithm we can view the weights associated with the particles zeta_p-1^1 ldots zeta_p-1^N as being W_p-1^1 ldots W_p-1^N where W_p-1^i propto G_p-1(zeta_p-1^i), and their relative effective sample size is rm rESS(W_p-1^1 ldots W_p-1^N).","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"In the SMC with adaptive resampling algorithm, the weight of a particle may be proportional to a product of many potential function values, depending on when the most recent normal resampling step was. In particular, whenever the standard resampling step is not taken, particles first inherit the weights of their ancestors and then multiply them by their own potential function values. The algorithm is as follows:","category":"page"},{"location":"smcadaptive/#Adaptive-resampling-algorithm-1","page":"Adaptive resampling","title":"Adaptive resampling algorithm","text":"","category":"section"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"Sample zeta_1^i oversetmathrmiidsim M_1 and compute W_1^i propto G_1(zeta_1^i) for iin 1 ldots N.\nFor p=2ldotsn:\ncompute mathcalE_p-1^N = rm rESS(W_p-1^1ldotsW_p-1^N).\nif p = n or mathcalE_p-1^N leq tau set R_p-1 leftarrow 1; otherwise set R_p-1 leftarrow 0.\nif R_p-1 = 1, sample a vector A_p-1^1 ldots A_p-1^N of i.i.d. rm Categorical(W_p-1^1 ldots W_p-1^N) random variables in increasing order; otherwise set (A_p-1^1 ldots A_p-1^N) = left(1 ldots N right).\nsample zeta_p^i oversetmathrmindsim M_p(zeta_p-1^A_p-1^i cdot) and compute W_p^i propto left(W_p-1^i right)^mathbbI(R_p-1 = 0) G_p(zeta_p^i) for iin 1ldotsN.","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"Note that the time n-1 particles are always resampled, so that eta_n^N is always an unweighted particle approximation of eta_n. This is primarily an implementation detail.","category":"page"},{"location":"smcadaptive/#Particle-approximations-1","page":"Adaptive resampling","title":"Particle approximations","text":"","category":"section"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"We define","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"eta_p^N propto sum_i=1^N left(W_p-1^i right)^ mathbbI(R_p-1 = 0) delta_zeta_p^i qquad p in 1ldotsn","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"and","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"hateta_p^N propto sum_i=1^N W_p^i delta_zeta_p^i qquad p in 1ldotsn","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"Appropriate approximations of hatZ_1^N ldots hatZ_n^N are also well-defined, but tedious to display.","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"These particle approximations all enjoy the same theoretical properties stated for the standard SMC Algorithm, although the asymptotic variance maps sigma^2_p are generally different. In fact, adaptive resampling improves in certain scenarios the quality of SMC approximations; its effects have been studied theoretically by Del Moral et al. (2010) and Whiteley et al. (2016).","category":"page"},{"location":"smcadaptive/#Variance-estimation-1","page":"Adaptive resampling","title":"Variance estimation","text":"","category":"section"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"All of the methods described in Variance estimators can be run on output from the SMC with adaptive resampling algorithm.","category":"page"},{"location":"smcadaptive/#","page":"Adaptive resampling","title":"Adaptive resampling","text":"One should be aware that the length of the vector returned by SequentialMonteCarlo.vpns will be of length m = 1 + sum_i=1^n-1 R_i leq n. This is a consequence of the fact that resampling only at certain times can be viewed as running an SMC algorithm with modified Markov kernels and potential functions defined on a more complicated state space –- the details are not presented here –- so that the number of terms in the asymptotic variance decomposition is m and not (necessarily) n.","category":"page"},{"location":"performance/#Performance-tips-1","page":"Performance tips","title":"Performance tips","text":"","category":"section"},{"location":"performance/#","page":"Performance tips","title":"Performance tips","text":"The majority of the computation involved in running SMC is the computation of the mutation and log potential functions. The goal in the design of this package has been to allow users to specify these functions in ways that avoid unnecessary computations and memory allocations.","category":"page"},{"location":"performance/#","page":"Performance tips","title":"Performance tips","text":"For example, user-defined scratch space structs are intended to allow users to avoid any memory allocations entirely. Running the algorithm in serial is possible with no allocations whatsoever. Example scratch space structs are defined in the multivariate linear Gaussian model and Lorenz96 models in SMCExamples.","category":"page"},{"location":"performance/#","page":"Performance tips","title":"Performance tips","text":"It is sometimes possible to store important per-particle information as part of the data associated with a particle. This is done, e.g., in the SMC sampler demo in SMCExamples to store two different log densities of the state, which can therefore be computed only once.","category":"page"},{"location":"performance/#","page":"Performance tips","title":"Performance tips","text":"In general, use of StaticArrays.jl can provide dramatic improvements for multivariate particles, and is highly recommended for fixed-size vector components of particles.","category":"page"},{"location":"intro/#intro-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"Sequential Monte Carlo (SMC) algorithms are defined in terms of an initial distribution M_1, a sequence of Markov transition kernels M_2 ldots M_n and a sequence of non-negative potential functions G_1 ldots G_n.","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"These in turn define a sequence of unnormalized distributions, or measures, specified in SMC Integrals. Integrals with respect to these measures are the principal quantities that an SMC algorithm provides approximations of.","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"The purpose of this package is to provide a very light interface to an SMC implementation with good multi-threaded performance. The main computationally intensive tasks in an SMC algorithm involve simulating from M_1 ldots M_n and evaluating G_1 ldots G_n a very large number of times: this package allows users to write these functions in Julia, which is a relatively high-level language, with little to no cost to performance. This is certainly an improvement over the very obsolete code written for Lee et al. (2010), which had good performance on GPUs but was almost impossible to make generally usable. It hopefully complements Murray (2013)'s LibBi software, which is focused on Bayesian inference in the context of general state-space hidden Markov models, and which achieves genericity via the heavier interface of a custom modelling language and a high-performance back end.","category":"page"},{"location":"intro/#Which-SMC-algorithm?-1","page":"Introduction","title":"Which SMC algorithm?","text":"","category":"section"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"The basic algorithm presented and implemented here was proposed by Stewart & McCarty Jr (1992), Gordon et al. (1993) and Kitagawa (1993). It is known as SMC with multinomial resampling and also often referred to as a Particle Filter. There are other SMC algorithms, some of which may be implemented in the future. The SMC algorithm with multinomial resampling is theoretically very well understood, and has associated variance estimators that are by-products of running the algorithm itself.","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"For readers interested in an SMC tutorial or methodological survey, two recent and complementary book chapters are Doucet & Johansen (2011) and Doucet & Lee (2018), which make some attempt at being comprehensive in various ways. The latter uses notation very similar to here, which is ultimately due to Pierre Del Moral. One minor notational difference here is the use of 1-indexing for sequences so as to be consistent with Julia's indexing. There are many other tutorials and surveys available.","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"Web pages maintained by Arnaud Doucet and Pierre del Moral provide many important references to methodological, theoretical and applied work.","category":"page"},{"location":"intro/#Notation-and-assumptions-1","page":"Introduction","title":"Notation and assumptions","text":"","category":"section"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"Let (mathsfX mathcalX) be a measurable space. The initial distribution M_1 is a probability measure on this space, and the Markov kernels M_2 ldots M_n evolve on this space. Every measure defined here, unless explicitly stated, will be a measure on this space. The non-negative functions G_1 ldots G_n have domain mathsfX. Every function f  mathsfX rightarrow mathbbR will be assumed to be measurable.","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"Weighting: if mu is a measure and gmathsfX rightarrow mathbbR_+ a non-negative function, then mu cdot g is the measure","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"(mucdot g)(A) = int_Amu(rm dx)g(x) qquad A in mathcalX","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"One can think of mu cdot g as mu weighted by g.","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"Mutation: if mu is a measure and P is a Markov kernel then mu P is the measure defined by","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"mu P(A)= int_mathsfX mu(rm dx) P(xA) qquad A in mathcalX","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"When mu is a probability measure, mu P(A) = Pr(Yin A) when Ysim P(Xcdot) and X sim mu. One can think of mu P as mu mutated by the Markov transition kernel P.","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"Integration: if mu is a measure and fmathsfXrightarrowmathbbR then","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"mu(f) = int_mathsfXf(x)mu(rm dx)","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"We denote by delta_x the Dirac measure centred at x. Note that delta_x(f) = f(x).\nA random variable X has a Categorical(a_1ldotsa_m) distribution if","category":"page"},{"location":"intro/#","page":"Introduction","title":"Introduction","text":"Pr(X=i) = fraca_isum_j=1^m a_j mathbf1_1ldotsm(i)","category":"page"},{"location":"smctheory/#maintheory-1","page":"Theoretical properties","title":"Theoretical properties of particle approximations","text":"","category":"section"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"All convergence results below involve taking limits as Nrightarrowinfty. Convergence almost surely is denoted oversetasrightarrow, convergence in probability is denoted oversetPrightarrow and weak convergence or convergence in distribution/law is denoted oversetLrightarrow.","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"The following Theorem provides some justification for the use of the main particle approximations; these now classical results can be deduced from various results of Del Moral (2004). We present the results only for the \"hatted\" or \"updated\" quantities to avoid repetition; analogous hold for their \"unhatted\" counterparts with a different sequence of maps sigma_1^2 ldots sigma_n^2.","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"Theorem [Del Moral, 2004]. Let the potential functions G_1 ldots G_n be bounded and strictly positive. There exist maps hatsigma_1^2 ldots hatsigma_n^2 such that the following hold for an arbitrary, bounded f:","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"Lack-of-bias: mathbbE left hatgamma_p^N(f) right = hatgamma_p(f) for all N geq 1.\nConsistency: hatgamma_p^N(f)oversetasrightarrowhatgamma_p(f) and hateta_p^N(f)oversetasrightarrowhateta_p(f).\nAsymptotic variance and mean-squared error (MSE):","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"N rm var left  hatgamma_p^N(f)  hatgamma_p(1) right  rightarrow hatsigma_p^2(f)","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"and","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"N mathbbE left left hateta_p^N(f) - hateta_p(f) right ^2 right rightarrow hatsigma_p^2(f-hateta_p(f))","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"Central Limit Theorems:","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"sqrtN left( hatgamma_p^N(f)  hatgamma_p(1) - eta_p^N(f) right) oversetLrightarrow N(0hatsigma_p^2(f))","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"and","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"sqrtN left( hateta_p^N(f) - eta_p^N(f) right) oversetLrightarrow N(0hatsigma_p^2(f - hateta_p(f)))","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"","category":"page"},{"location":"smctheory/#Note-on-the-sorted-ancestor-indices-1","page":"Theoretical properties","title":"Note on the sorted ancestor indices","text":"","category":"section"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"The theoretical results above are typically proven for an algorithm that differs very slightly from the SMC algorithm implemented here. In particular, one would usually analyze the algorithm by considering A_p-1^1ldotsA_p-1^N to be i.i.d. rm Categorical (G_p-1(zeta_p-1^1) ldots G_p-1(zeta_p-1^N)) random variables rather than being in sorted order.","category":"page"},{"location":"smctheory/#","page":"Theoretical properties","title":"Theoretical properties","text":"The laws of the approximations hatZ_p^N, eta_p^N(f) and hateta_p^N(f), however, are invariant to permutations of the indices of the ancestors A_p-1^1ldotsA_p-1^N in the algorithm. Therefore, the sorting of the ancestor indices may be regarded as an implementation issue that does not affect the particle approximations themselves.","category":"page"},{"location":"hmm/#Connection-to-hidden-Markov-models-1","page":"Hidden Markov models","title":"Connection to hidden Markov models","text":"","category":"section"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"Probably the most common use for SMC is in statistical applications involving hidden Markov models (HMMs), where the methodology originates. Indeed, there is a very simple correspondence between an HMM and an SMC model. The purpose of this part of the documentation is only to clarify how different SMC models can be associated with the same HMM.","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"An simple HMM can be described as a bivariate Markov chain (X_1Y_1) ldots (X_n Y_n) as follows. X_1 ldots X_n is a Markov chain determined by an initial distribution mu and Markov transition densities f_2 ldots f_n. For each p in 1 ldots n, Y_p is conditionally independent of all other random variables given X_p and Y_p mid (X_p = x) has density g_p(x  cdot). Statistical inference for HMMs involve performing inference after observing (y_1 ldots y_n) as a realization of (Y_1 ldots Y_n).","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"A standard use of SMC for such an HMM is as follows. Let the observations (y_1 ldots y_n) be given and choose M_1 = mu, M_p(x dx) = f_p(x x) dx for p in 2 ldots n and G_p(x) = g_p(x y_p) for p in 1 ldots n. Notice that the potential function G_p is defined using the observation y_p. Then it can be deduced that hatZ_p is the marginal likelihood associated with the first p observations, eta_p is the predictive distribution of X_p given the first p-1 observations and hateta_p is the filtering distribution of X_p given the first p observations. That is,","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"X_p mid (Y_0 ldots Y_p-1) = (y_0 ldots y_p-1) sim eta_p","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"and","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"X_p mid (Y_0 ldots Y_p) = (y_0 ldots y_p) sim hateta_p","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"However, this is only one way to map the HMM into an SMC Model. We extend the HMM's state space and define M_1(d(x_rm prev x)) = mu(dx) delta_x(dx_rm prev), and","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"M_p((x_rm prevx) d(x_rm prevx)) = q_p(x x) delta_x(dx_rm prev) qquad p in 2 ldots n","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"where q_p is such that q_p(x x)  0 whenever f_p(x x)  0. Then one can define","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"G_p(x_rm prev x) = fracf_p(x_rm prev x)q_p(x_rm prev x) g(x y_p) qquad p in 1 ldots n","category":"page"},{"location":"hmm/#","page":"Hidden Markov models","title":"Hidden Markov models","text":"and it is still the case that hatZ_p is the marginal likelihood associated with the first p observations, eta_p is the predictive distribution of X_p given the first p-1 observations and hateta_p is the filtering distribution of X_p given the first p observations. An example of using different SMC models for a given HMM is provided in SMCExamples: comparison of Linear Gaussian models ; the model associated with the \"locally optimal proposal\" is defined here.","category":"page"},{"location":"smcve/#vepage-1","page":"Variance estimators","title":"Variance estimators","text":"","category":"section"},{"location":"smcve/#Asymptotic-variances-1","page":"Variance estimators","title":"Asymptotic variances","text":"","category":"section"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"We recall that the particle approximations have asymptotic variances associated with particular test functions f. Letting n be arbitrary to avoid additional indexing notation, the values sigma_n^2(f) and hatsigma_n^2(f) are typically not possible to calculate exactly. They can be decomposed into sums as follows:","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"sigma_n^2(f) = sum_p=1^n v_pn(f) qquad hatsigma_n^2(f) = sum_p=1^n hatv_pn(f)","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"The quantities v_pn(f) and hatv_pn(f) also cannot be computed exactly in general, but this decomposition can in some circumstances shed some light on the nature of the approximation errors.","category":"page"},{"location":"smcve/#The-estimators-1","page":"Variance estimators","title":"The estimators","text":"","category":"section"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"Most Monte Carlo approximations are accompanied by Monte Carlo approximations of their asymptotic variance or MSE. Lee and Whiteley (2015) defined V_p^N(f), computable using the simulated particle system, allowing one to approximate asymptotic variances of interest.","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"approximation approximation of\nV_p^N(f) rm var left  gamma_p^N(f)gamma_p(1) right \nV_p^N(f-eta_p^N(f)) mathbbE left left eta_p^N(f) - eta_p(f) right^2 right\nhatV_p^N(f) rm var left hatgamma_p^N(f)  hatgamma_p(1) right\nhatV_p^N(f-hateta_p^N(f)) mathbbE left left hateta_p^N(f) - hateta_p(f) right ^2right","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"These approximations have the following theoretical justification, particularly for their approximation of the asymptotic variance and MSE maps defined earlier. Again, we present the results only for the \"updated\" quantities to avoid repetition; analogous results hold for their \"unhatted\" counterparts.","category":"page"},{"location":"smcve/#Theoretical-justification-1","page":"Variance estimators","title":"Theoretical justification","text":"","category":"section"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"The variance approximations have consistency properties, and some of them have lack-of-bias properties.","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"Theorem [Lee & Whiteley, 2015]. Let the potential functions G_1 ldots G_n be bounded and strictly positive. The following hold for an arbitrary, bounded f:","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"Lack-of-bias: mathbbEleft(hatZ_p^N)^2hatV_p^N(f)right=rm varleft hatgamma_p(f)right for all N geq 1.\nConsistency: NhatV_p^N(f) oversetPrightarrow hatsigma_p^2(f) and N hatV_p^N(f-hateta_p^N(f)) oversetPrightarrow hatsigma_p^2(f - hateta_p(f)).","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"Remark. A consistent estimator of the asymptotic MSE of eta_p(f) or hateta_p(f) was proposed in Chan & Lai (2013). This is very similar to that of Lee & Whiteley (2015) but does not satisfy the lack-of-bias property.","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"Lee & Whiteley (2015) also define approximations v_pn^N(f) and hatv_pn^N(f) of v_pn(f) and hatv_pn(f), respectively. Their sums v_n^N(f)=sum_p=1^nv_pn^N(f) and hatv_n^N(f)=sum_p=1^nhatv_pn^N(f) can also be used as alternative approximations of sigma_n^2(f) and hatsigma_n^2(f)","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"Theorem [Lee & Whiteley, 2015]. Let the potential functions G_1 ldots G_n be bounded and strictly positive. The following hold for an arbitrary, bounded f:","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"Lack-of-bias: mathbbEleftleft(hatZ_n^Nright)^2hatv_pn^N(f)right=hatZ_n^2hatv_pn(f) for all Ngeq1.\nConsistency: hatv_pn^N(f)oversetPrightarrowhatv_pn(f) and hatv_pn^N(f)(f-hateta_n^N(f))oversetPrightarrowhatv_pn(f-hateta_n(f)).\nLack-of-bias: mathbbEleftleft(hatZ_n^Nright)^2hatv_n^N(f)right=hatZ_n^2hatsigma_n^2(f) for all Ngeq1.\nConsistency: hatv_n^N(f)oversetPrightarrowhatsigma_n^2(f) and hatv_n^N(f-hateta_n^N(f))oversetPrightarrowhatsigma_n^2(f-hateta_n(f)).","category":"page"},{"location":"smcve/#","page":"Variance estimators","title":"Variance estimators","text":"","category":"page"},{"location":"refs/#References-1","page":"References","title":"References","text":"","category":"section"},{"location":"refs/#","page":"References","title":"References","text":"C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods. J. R. Stat. Soc. Ser. B Stat. Methodol., 72(3):269–342, 2010.\nC. Andrieu, A. Lee, and M. Vihola. Uniform ergodicity of the iterated conditional SMC and geometric ergodicity of particle Gibbs samplers. Bernoulli, 24(2):842–-872, 2018.\nH. P. Chan and T. L. Lai. A general theory of particle filters in hidden Markov models and some applications. Ann. Statist. 41(6):2877–2904, 2013.\nN. Chopin and S. S. Singh. On particle Gibbs sampling. Bernoulli, 21(3):1855–-1883, 2015.\nP. Del Moral. Feynman–Kac formulae: genealogical and interacting particle systems with applications. Springer-Verlag, 2004.\nP. Del Moral, A. Doucet, and A. Jasra. On adaptive resampling procedures for sequential Monte Carlo methods. Bernoulli, 18(1):252–278, 2012.\nL. Devroye. Non-uniform random variate generation. Springer-Verlag, 1986.\nA. Doucet and A. M. Johansen. A tutorial on particle filtering and smoothing: Fifteen years later. In D. Crisan and B. Rozovsky, eds., The Oxford Handbook of Nonlinear Filtering, pp. 656–-704. Oxford University Press, 2011.\nA. Doucet and A. Lee.  Sequential Monte Carlo methods. In M. Drton, S. Lauritzen, M. Maathuis, and M. Wainwright, eds., Handbook of Graphical Models. 2018. In press.\nN. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-Gaussian Bayesian state estimation. Radar and Signal Processing, IEE Proceedings F, 140(2):107–-113, 1993.\nW. Hörmann. The generation of binomial random variates. J. Stat. Comput. Simul., 46(1–2):101–-110, 1993.\nG. Kitagawa. A Monte Carlo filtering and smoothing method for non-Gaussian nonlinear state space models. In Proceedings of the 2nd US-Japan Joint Seminar on Statistical Time Series Analysis, pp. 110–131, 1993.\nA. Kong, J. S. Liu, and W. H. Wong. Sequential imputations and Bayesian missing data problems. J. Am. Stat. Assoc., 89(425):278–288, 1994.\nA. Lee and N. Whiteley. Variance estimation in the particle filter. arXiv:1509.00394, 2015.\nA. Lee, C. Yau, M. B. Giles, A. Doucet, and C. C. Holmes. On the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods. J. Comput. Graph. Statist., 19(4):769–-789, 2010.\nF. Lindsten, R. Douc, and E. Moulines. Uniform ergodicity of the particle Gibbs sampler. Scand. J. Statist., 42(3):775-–797, 2015.\nJ. S. Liu and R. Chen. Blind deconvolution via sequential imputations. J. Am. Stat. Assoc., 90:567–-576, 1995.\nD. Lurie and H. O. Hartley. Machine-generation of order statistics for Monte Carlo computations. Am. Stat., 26(1):26–-27, 1972.\nL. M. Murray. Bayesian state-space modelling on high-performance hardware using LibBi. J. Stat. Softw. 67(10):1–36, 2015.\nL. Stewart and P. McCarty Jr. Use of Bayesian belief networks to fuse continuous and discrete information for target recognition, tracking, and situation assessment. In Aerospace Sensing, pp. 177-–185. International Society for Optics and Photonics, 1992.\nN. Whiteley, A. Lee, and K. Heine. On the role of interaction in sequential Monte Carlo algorithms. Bernoulli, 22(1):494–-529, 2016.","category":"page"},{"location":"guide/#Documentation-strings-1","page":"Types and functions","title":"Documentation strings","text":"","category":"section"},{"location":"guide/#Types-1","page":"Types and functions","title":"Types","text":"","category":"section"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SMCModel{F1<:Function,F2<:Function}","category":"page"},{"location":"guide/#SequentialMonteCarlo.SMCModel","page":"Types and functions","title":"SequentialMonteCarlo.SMCModel","text":"SMCModel(M!::F1, lG::F2, maxn::Int64, particle::Type, pScratch::Type) where\n  {F1<:Function,F2<:Function}\n\nM! Mutation function\nlG Log potential function\nmaxn Maximum n for which the model is well-defined\nparticle Type of a particle\npScratch Type of particle scratch space\n\n\n\n\n\n","category":"type"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SMCIO{Particle, ParticleScratch}","category":"page"},{"location":"guide/#SequentialMonteCarlo.SMCIO","page":"Types and functions","title":"SequentialMonteCarlo.SMCIO","text":"SMCIO{Particle, ParticleScratch}\n\nStructs of this type should be constructed using the provided constructor. Important fields:\n\nN::Int64 Number of particles N\nn::Int64 Number of steps n\nnthreads::Int64 Number of threads\nfullOutput::Bool Whether particle system history should be recorded\nessThreshold::Float64 Relative ESS Threshold tau\nzetas::Vector{Particle} Time n particles zeta_n^1 ldots zeta_n^N\neves::Vector{Int64} Time n Eve indices E_n^1 ldots E_n^N\nws::Vector{Float64} Time n weights W_n^1 ldots W_n^N\nlogZhats::Vector{Float64} log(hatZ^N_1) ldots log(hatZ^N_n)\nVhat1s::Vector{Float64} hatV_1^N(1) ldots hatV_n^N(1)\nesses::Vector{Float64} Relative ESS values mathcalE_1^N ldots mathcalE_n^N\nresample::Vector{Bool} Resampling indicators R_1 ldots R_n-1\n\nPopulated only if fullOutput == true\n\nallZetas::Vector{Vector{Particle}} All the particles\nallWs::Vector{Vector{Float64}} All the weights\nallAs::Vector{Vector{Int64}} All the ancestor indices\nallEves::Vector{Vector{Int64}} All the Eve indices\n\n\n\n\n\n","category":"type"},{"location":"guide/#Functions-1","page":"Types and functions","title":"Functions","text":"","category":"section"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"smc!(model::SMCModel, smcio::SMCIO)","category":"page"},{"location":"guide/#SequentialMonteCarlo.smc!-Tuple{SMCModel,SMCIO}","page":"Types and functions","title":"SequentialMonteCarlo.smc!","text":"smc!(model::SMCModel, smcio::SMCIO)\n\nRun the SMC algorithm for the given model and input/output arguments.\n\nIf smcio.nthreads == 1 the algorithm will run in serial.\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"csmc!(model::SMCModel, smcio::SMCIO{Particle}, ref::Vector{Particle},\n  refout::Vector{Particle}) where Particle","category":"page"},{"location":"guide/#SequentialMonteCarlo.csmc!-Union{Tuple{Particle}, Tuple{SMCModel,SMCIO{Particle,ParticleScratch} where ParticleScratch,Array{Particle,1},Array{Particle,1}}} where Particle","page":"Types and functions","title":"SequentialMonteCarlo.csmc!","text":"csmc!(model::SMCModel, smcio::SMCIO, ref::Vector{Particle}, refout::Vector{Particle})\n\nRun the conditional SMC algorithm for the given model, input/output arguments, reference path and output path.\n\nIt is permitted for ref and refout to be the same. If smcio.nthreads == 1 the algorithm will run in serial.\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SMCIO{Particle, ParticleScratch}(N::Int64, n::Int64, nthreads::Int64,\n  fullOutput::Bool, essThreshold::Float64 = 2.0) where {Particle,\n  ParticleScratch}","category":"page"},{"location":"guide/#SequentialMonteCarlo.SMCIO-Union{Tuple{Int64,Int64,Int64,Bool}, Tuple{Int64,Int64,Int64,Bool,Float64}, Tuple{ParticleScratch}, Tuple{Particle}} where ParticleScratch where Particle","page":"Types and functions","title":"SequentialMonteCarlo.SMCIO","text":"SMCIO{Particle, ParticleScratch}(N::Int64, n::Int64, nthreads::Int64,\n  fullOutput::Bool, essThreshold::Float64 = 2.0) where\n  {Particle, ParticleScratch}\n\nConstructor for SMCIO structs.\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SequentialMonteCarlo.eta(smcio::SMCIO{Particle}, f::F, hat::Bool,\n  p::Int64) where {Particle, F<:Function}","category":"page"},{"location":"guide/#SequentialMonteCarlo.eta-Union{Tuple{F}, Tuple{Particle}, Tuple{SMCIO{Particle,ParticleScratch} where ParticleScratch,F,Bool,Int64}} where F<:Function where Particle","page":"Types and functions","title":"SequentialMonteCarlo.eta","text":"eta(smcio::SMCIO{Particle}, f::F, hat::Bool, p::Int64) where {Particle, F<:Function}\n\nCompute:\n\n!hat: eta^N_p(f)\nhat:  hateta_p^N(f)\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SequentialMonteCarlo.allEtas(smcio::SMCIO, f::F, hat::Bool) where F<:Function","category":"page"},{"location":"guide/#SequentialMonteCarlo.allEtas-Union{Tuple{F}, Tuple{SMCIO,F,Bool}} where F<:Function","page":"Types and functions","title":"SequentialMonteCarlo.allEtas","text":"allEtas(smcio::SMCIO, f::F, hat::Bool) where F<:Function\n\nCompute eta(smcio::SMCIO, f::F, hat::Bool, p) for p in {1, …, smcio.n}\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SequentialMonteCarlo.slgamma(smcio::SMCIO, f::F, hat::Bool, p::Int64) where\n  F<:Function","category":"page"},{"location":"guide/#SequentialMonteCarlo.slgamma-Union{Tuple{F}, Tuple{SMCIO,F,Bool,Int64}} where F<:Function","page":"Types and functions","title":"SequentialMonteCarlo.slgamma","text":"slgamma(smcio::SMCIO, f::F, hat::Bool, p::Int64) where {Particle, F<:Function}\n\nCompute:\n\n!hat: (eta^N_p(f) geq 0 log gamma^N_p(f))\nhat:  (hateta^N_p(f) geq 0 log hatgamma_p^N(f))\n\nThe result is returned as a Tuple{Bool, Float64}: the first component represents whether the returned value is non-negative, the second is the logarithm of the absolute value of the approximation.\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SequentialMonteCarlo.allGammas(smcio::SMCIO, f::F, hat::Bool) where F<:Function","category":"page"},{"location":"guide/#SequentialMonteCarlo.allGammas-Union{Tuple{F}, Tuple{SMCIO,F,Bool}} where F<:Function","page":"Types and functions","title":"SequentialMonteCarlo.allGammas","text":"allGammas(smcio::SMCIO, f::F, hat::Bool) where F<:Function\n\nCompute slgamma(smcio::SMCIO, f::F, hat::Bool, p) for p in {1, …, smcio.n}\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SequentialMonteCarlo.V(smcio::SMCIO{Particle}, f::F, hat::Bool, centred::Bool,\n  p::Int64) where {Particle, F<:Function}","category":"page"},{"location":"guide/#SequentialMonteCarlo.V-Union{Tuple{F}, Tuple{Particle}, Tuple{SMCIO{Particle,ParticleScratch} where ParticleScratch,F,Bool,Bool,Int64}} where F<:Function where Particle","page":"Types and functions","title":"SequentialMonteCarlo.V","text":"V(smcio::SMCIO{Particle}, f::F, hat::Bool, centred::Bool, p::Int64) where\n  {Particle, F<:Function}\n\nCompute:\n\n!hat & !centred: V^N_p(f)\n!hat & centred: V^N_p(f-eta_p^N(f))\nhat & !centred: hatV_p^N(f)\nhat & centred:  hatV_p^N(f-hateta_p^N(f))\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SequentialMonteCarlo.vpns(smcio::SMCIO, f::F, hat::Bool, centred::Bool,\n  n::Int64) where F<:Function","category":"page"},{"location":"guide/#SequentialMonteCarlo.vpns-Union{Tuple{F}, Tuple{SMCIO,F,Bool,Bool,Int64}} where F<:Function","page":"Types and functions","title":"SequentialMonteCarlo.vpns","text":"vpns(smcio::SMCIO, f::F, hat::Bool, centred::Bool, n::Int64) where F<:Function\n\nCompute a vector of the values of, for p = 1, …, n,\n\n!hat & !centred: v^N_pn(f)\n!hat & centred:  v^N_pn(f-eta_n^N(f))\nhat & !centred:  hatv_pn^N(f)\nhat & centred:   hatv_pn^N(f-hateta_n^N(f))\n\nNote: if essThreshold <= 1.0, and resampling did not occur at every time, the length of the output will be less than n.\n\n\n\n\n\n","category":"method"},{"location":"guide/#","page":"Types and functions","title":"Types and functions","text":"SequentialMonteCarlo.v(smcio::SMCIO, f::F, hat::Bool, centred::Bool,\n  n::Int64) where F<:Function","category":"page"},{"location":"guide/#SequentialMonteCarlo.v-Union{Tuple{F}, Tuple{SMCIO,F,Bool,Bool,Int64}} where F<:Function","page":"Types and functions","title":"SequentialMonteCarlo.v","text":"v(smcio::SMCIO, f::F, hat::Bool, centred::Bool, n::Int64) where F<:Function\n\nCompute:\n\n!hat & !centred: v^N_n(f)\n!hat & centred: v^N_n(f-eta_n^N(f))\nhat & !centred: hatv_n^N(f)\nhat & centred:  hatv_n^N(f-hateta_n^N(f))\n\n\n\n\n\n","category":"method"},{"location":"csmc/#Conditional-SMC-1","page":"Conditional SMC","title":"Conditional SMC","text":"","category":"section"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"One relatively recent subtle modification of the SMC algorithm was proposed by Andrieu et al. (2010), and known as conditional SMC (cSMC). The algorithm is as follows:","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"Input: reference path (x_1^rm refldotsx_n^rm ref)","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"Set zeta_1^1=x_1^rm ref and sample zeta_1^ioversetmathrmiidsimM_1 for i in 2 ldots N .\nFor p=2ldotsn:\nset A_p-1^1=1 and zeta_p^1=x_p^rm ref.\nsample a vector A_p-1^2ldotsA_p-1^N of i.i.d. rm Categorical(G_p-1(zeta_p-1^1)ldotsG_p-1(zeta_p-1^N)) random variables in increasing order.\nsample zeta_p^i oversetmathrmindsim M_p(zeta_p-1^A_p-1^i cdot) for i in 2 ldots N .\nSample K_n sim rm Categorical(G_n(zeta_n^1)ldotsG_n(zeta_n^N)) and for p=n-1ldots1 set K_p=A_p^K_p+1.\nOutput the path (zeta_1^K_1ldotszeta_n^K_n).","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"cSMC is of special interest as it defines a Markov kernel that is ergodic, for N geq 2, with invariant probability measure given by","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"hatpi(A)=hatZ_n^-1int_AG_n(x_n)M_1(rm dx_1)prod_p=2^nG_p-1(x_p-1)M_p(x_p-1rm dx_p) qquad A in mathcalX^otimesn","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"That is, the Markov kernel P_N(x^rm ref cdot) defined by calling the cSMC algorithm has invariant probability measure hatpi. Theoretical results, in particular concerning the effect of N on the rate of convergence of P_N, have been developed by Chopin & Singh (2015), Andrieu et al. (2018) and Lindsten et al. (2015).","category":"page"},{"location":"csmc/#","page":"Conditional SMC","title":"Conditional SMC","text":"Conditional SMC may also be used with adaptive resampling as detailed in Adaptive resampling.","category":"page"},{"location":"smcalgorithm/#SMC-algorithm-and-particle-approximations-1","page":"SMC algorithm","title":"SMC algorithm and particle approximations","text":"","category":"section"},{"location":"smcalgorithm/#basicsmc-1","page":"SMC algorithm","title":"SMC algorithm","text":"","category":"section"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"The implemented algorithm is as follows. The random variables zeta_p^1 ldots zeta_p^N are typically referred to as particles, and the approximations they define are referred to as particle approximations.","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"Sample zeta_1^i oversetmathrmiidsim M_1 for i in 1ldotsN.\nFor p=2ldotsn:\nsample a vector A_p-1^1ldotsA_p-1^N of i.i.d. rm Categorical(G_p-1(zeta_p-1^1)ldotsG_p-1(zeta_p-1^N)) random variables in increasing order.\nsample zeta_p^i oversetmathrmind sim M_p(zeta_p-1^A_p-1^i cdot) for i in 1ldotsN.","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"This algorithm is referred to as SMC with multinomial resampling because the vector of numbers of offspring of the time p-1 particles follows a multinomial distribution.","category":"page"},{"location":"smcalgorithm/#mainapprox-1","page":"SMC algorithm","title":"Main particle approximations","text":"","category":"section"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"For some p in 1ldotsn, the particle approximation of eta_p(f) is obtained by constructing a particle approximation eta_p^N of eta_p and then calculating eta_p^N(f). Specifically, we define","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"eta_p^N = frac1N sum_i=1^N delta_zeta_p^i qquad p in 1ldotsn","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"and the particle approximation of eta_p(f) is then simply","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"eta_p^N(f) = int f(x) eta_p^N(rm dx) = frac1N sum_i=1^N f(zeta_p^i)","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"Similarly, we define","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"hateta_p^N= fraceta_p^N cdot G_peta_p^N(G_p) qquad p in 1 ldots n","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"so that the particle approximation of hateta_p(f) is","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"hateta^N_p(f) = int f(x) hateta_p^N(rm dx) fracsum_i=1^N G_p(zeta_p^i) f(zeta_p^i)sum_i=1^N G_p(zeta_p^i)","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"Finally, the particle approximations of hatZ_1ldotshatZ_n are defined by hatZ_1^N=eta_1^N(G_1) and for p in 2ldotsn, hatZ_p^N=hatZ_p-1^Neta_p^N(G_p).","category":"page"},{"location":"smcalgorithm/#Genealogical-structure-1","page":"SMC algorithm","title":"Genealogical structure","text":"","category":"section"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"The particle system defined by the SMC algorithm has a genealogical structure, induced by the ancestor indices. We define Eve indices such that E_p^i is the index of the time 1 ancestor of zeta_p^i. That is, the Eve indices satisfy E_1^i = i for i in 1 ldots N and","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"E_p^i = E_p-1^A_p-1^i qquad p in 2 ldots n quad i in 1 ldots N","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"These can be used to quantify and visualize the path degeneracy phenomenon associated with the SMC algorithm. Since the ancestor indices are in sorted order, so too are the Eve indices at each time.","category":"page"},{"location":"smcalgorithm/#Recursive-definition-1","page":"SMC algorithm","title":"Recursive definition","text":"","category":"section"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"The SMC algorithm is intimately connected to the recursive definition of the SMC measures themselves.","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"Let us define the sampling operator S^N to be the random map that takes a measure mu and outputs a random measure","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"mu^N = S^N mu = frac1N sum_i=1^N delta_X^i qquad textwhere  X^i oversetmathrmiidsim mu","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"Then we have","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"eta^N_1 = S^N M_1 qquad hateta_1^N = frac eta^N_1 cdot G_1 eta^N_1(G_1)","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"and for p = 2 ldots n,","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"eta^N_p = S^N (hateta_p-1^N M_p) qquad hateta_p^N = frac eta^N_p cdot G_p eta^N_p(G_p)","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"This mirrors almost exactly the recursive definitions for the SMC measures, where one has instead","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"eta_1 = M_1 qquad hateta_1 = frac eta_1 cdot G_1 eta_1(G_1)","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"and for p = 2 ldots n,","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"eta_p = hateta_p-1 M_p qquad hateta_p = frac eta_p cdot G_p eta_p(G_p)","category":"page"},{"location":"smcalgorithm/#","page":"SMC algorithm","title":"SMC algorithm","text":"We notice also that hatZ^N_p = hatZ^N_p-1 eta^N_p(G_p) while hatZ_p = hatZ_p-1 eta_p(G_p) for p in 2 ldots n.","category":"page"},{"location":"impl/#Implementation-notes-1","page":"Implementation notes","title":"Implementation notes","text":"","category":"section"},{"location":"impl/#","page":"Implementation notes","title":"Implementation notes","text":"The multi-threaded implementation of SMC is fairly straightforward. The attempt to efficiently utilize multiple cores is largely focused around using appropriate data structures and low-overhead resampling / selection, since the remaining steps are embarrassingly parallel.","category":"page"},{"location":"impl/#","page":"Implementation notes","title":"Implementation notes","text":"There is also some attention paid to Julia's threading interface and, for both parallel and serial execution, the need to avoid dynamic memory allocations by pre-allocating reusable memory. All code was written using Julia's core and standard library, with some code written in more general purpose packages such as NonUniformRandomVariateGeneration.jl.","category":"page"},{"location":"impl/#","page":"Implementation notes","title":"Implementation notes","text":"Generating the ancestor indices in sorted order is accomplished primarily through use of the uniform spacings method for generating a sequence of sorted uniform random variates developed by Lurie & Hartley (1972) and described by Devroye (1986, p. 214). In multi-threaded code, the multinomial variate giving the number of ancestor indices each thread should simulate using a thread-specific subvector of particle weights is simulated by using a combination of inversion sampling and an implementation of the btrd algorithm of Hörmann (1993), which simulates binomially distributed variates in expected mathcalO(1) time. Simulating a multinomial random variate is accomplished by simulating a sequence of appropriate Binomial random variates. The code-generating function for copying particles was adapted from a suggestion on Julia Discourse by Greg Plowman.","category":"page"},{"location":"impl/#","page":"Implementation notes","title":"Implementation notes","text":"Thread-specific, counter-based RNGs in the RandomNumbers.jl package are used. These are only slightly slower than the MersenneTwister RNG provided by Julia's standard library Random.","category":"page"},{"location":"smcinterface/#interface-1","page":"SMC interface","title":"Interface","text":"","category":"section"},{"location":"smcinterface/#Specifying-an-SMC-model-1","page":"SMC interface","title":"Specifying an SMC model","text":"","category":"section"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"We recall from the Introduction that the SMC algorithm is defined in terms of M_1 ldots M_n and G_1 ldots G_n.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"A model of type SMCModel can be created by calling","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"model = SMCModel(M!, lG, maxn::Int64, Particle, ParticleScratch)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"where Particle and ParticleScratch are user-defined types, M! is a void function","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"M!(newParticle::Particle, rng::RNG, p::Int64, particle::Particle,\n  scratch::ParticleScratch)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"and lG is a function returning a Float64","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"lG(p::Int64, particle::Particle, scratch::ParticleScratch)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The RNG type is defined by the RNGPool package.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"There is a correspondence between the function M! and M_1 ldots M_n: calling M!(x', rng, p, x, scratch), should make x a realization of a sample from M_p(x cdot) with the convention that M_1(xcdot) = M_1(cdot) for any x. Similarly, lG and G_1 ldots G_n correspond in that lG(p,x) $ = \\log G_p(x)$. Logarithms are used to avoid numerical issues. maxn is the maximum value of n for which M! and lG are well-defined; users may choose to run the SMC algorithm for any integer value of n up to and including maxn.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The types Particle and ParticleScratch must have constructors that take no arguments. One may choose ParticleScratch = Nothing, in which case nothing will be passed to M! and lG. Using scratch space is optional but can significantly improve performance in certain scenarios; it provides a mechanism for users to avoid dynamic memory allocations in M! and/or lG. This scratch space will be used by every particle associated with a given thread. A thread-specific pseudo-random number generator (RNG) rng will be passed to the M! function by the algorithm, and should be used in lieu of Julia's global RNG.","category":"page"},{"location":"smcinterface/#Running-the-SMC-algorithm-1","page":"SMC interface","title":"Running the SMC algorithm","text":"","category":"section"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The SMC algorithm is run by calling","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"smc!(model::SMCModel, smcio::SMCIO)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The second argument, smcio, is a struct containing inputs and outputs for the smc! algorithm. It is straightforward to construct and involves specifying the number of particles N, the number of iterations n, the number of threads nthreads, whether the entire history of the particle system should be recorded fullOutput, and an effective sample size threshold essThreshold that is explained on the adaptive resampling page, and can be safely ignored on a first reading. smcio can be created by calling","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"smcio = SMCIO{model.particle, model.pScratch}(N::Int64, n::Int64,\n  nthreads::Int64, fullOutput::Bool, essThreshold::Float64 = 2.0)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"Currently N must be an integer multiple of nthreads; this may change in the future.","category":"page"},{"location":"smcinterface/#Extracting-the-main-outputs-1","page":"SMC interface","title":"Extracting the main outputs","text":"","category":"section"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"A vector of approximations (loghatZ_1^N ldots loghatZ_n^N) is stored in smcio.logZhats, which can be used in conjunction with SequentialMonteCarlo.eta to produce the approximations gamma_p^N(f) or hatgamma_p^N(f).","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"One can extract the approximation eta_p^N(f) or hateta_p^N(f) by calling","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"SequentialMonteCarlo.eta(smcio, f, hat::Bool, p::Int64)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"with hat determining which approximation is returned. Calling this function with p < smcio.n requires smcio.fullOutput = true.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"One can extract the approximations (eta^N_p(f) geq 0 log gamma^N_p(f)) or (hateta^N_p(f) geq 0 log hatgamma^N_p(f)) by calling","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"SequentialMonteCarlo.slgamma(smcio, f, hat::Bool, p::Int64)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"with hat determining which approximation is returned. Calling this function with p < smcio.n requires smcio.fullOutput = true.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"If smcio.fullOutput = true one can call SequentialMonteCarlo.allEtas or SequentialMonteCarlo.allGammas to obtain a vector of outputs of SequentialMonteCarlo.eta or SequentialMonteCarlo.slgamma, respectively.","category":"page"},{"location":"smcinterface/#Variance-estimators-1","page":"SMC interface","title":"Variance estimators","text":"","category":"section"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The following functions relate to the approximations detailed in the variance estimators page.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The function","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"SequentialMonteCarlo.V(smcio, f, hat::Bool, centred::Bool, p::Int64)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"returns:","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"hat centred approximation approximation of\nfalse false V_p^N(f) rm var left  gamma_p^N(f)gamma_p(1) right \nfalse true V_p^N(f-eta_p^N(f)) mathbbEleftleft eta_p^N(f)-eta_p(f)right ^2right\ntrue false hatV_p^N(f) rm varleft hatgamma_p^N(f)hatgamma_p(1)right\ntrue true hatV_p^N(f-hateta_p^N(f)) mathbbEleftleft hateta_p^N(f)-hateta_p(f)right ^2right","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"Calling SequentialMonteCarlo.V with p < smcio.n requires smcio.fullOutput = true.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"A vector of the quantities hatV_1^N(1)ldotshatV_n^N(1) is stored in smcio.Vhat1s. These are approximations of the relative variances of hatZ_1^NldotshatZ_n^N = exp.(smcio.logZhats), i.e. the variances of hatZ_1^NhatZ_1ldotshatZ_n^NhatZ_n.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"When smcio.fullOutput = true, a vector of the approximations v^N_pn(f) or hatv^N_pn(f) for p in 1ldotsn can be obtained by calling","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"SequentialMonteCarlo.vpns(smcio, f, hat::Bool, centred::Bool, n::Int64)","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"One can choose n < smcio.n. If only the sum of one of these vectors is desired, i.e. v_n^N(f) or hatv^N_n(f), this can be obtained be calling","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"SequentialMonteCarlo.v(smcio, f, hat::Bool, centred::Bool, n::Int64)","category":"page"},{"location":"smcinterface/#Adaptive-resampling-1","page":"SMC interface","title":"Adaptive resampling","text":"","category":"section"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The adaptive resampling mechanism is activated by choosing essThreshold <= 1.0. A vector of Bool values indicating whether or not resampling took place at each time can be accessed as smcio.resample, which has length smcio.n - 1 and is corresponds exactly to the random variables R_1 ldots R_n-1 defined in the SMC with adaptive resampling algorithm.","category":"page"},{"location":"smcinterface/#Accessing-the-particle-system-1","page":"SMC interface","title":"Accessing the particle system","text":"","category":"section"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"If smcio.fullOutput == true, one can access:","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"name value\nsmcio.allZetas[p] zeta_p^1 ldots zeta_p^N\nsmcio.allWs[p] propto G_p(zeta_p^1) ldots G_p(zeta_p^N)\nsmcio.allEves[p] E_p^1 ldots E_p^N\nsmcio.allAs[p] A_p^1 ldots A_p^N","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"Note that smcio.allAs has length smcio.n-1 while the others in the table above have length smcio.n.","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"Even if smcio.fullOutput == false, one can access:","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"name value\nsmcio.zetas zeta_n^1 ldots zeta_n^N\nsmcio.ws propto G_n(zeta_n^1)ldotsG_n(zeta_n^N)\nsmcio.eves E_n^1 ldots E_n^N\nsmcio.esses mathcalE_1^N ldots mathcalE_n^N","category":"page"},{"location":"smcinterface/#Conditional-SMC-1","page":"SMC interface","title":"Conditional SMC","text":"","category":"section"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"The cSMC algorithm can be called as follows:","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"csmc!(model::SMCModel, smcio::SMCIO, ref::Vector{Particle},\n  refout::Vector{Particle})","category":"page"},{"location":"smcinterface/#","page":"SMC interface","title":"SMC interface","text":"where ref is the input reference path and refout the output path. It is permitted for ref and refout to be the same vector.","category":"page"},{"location":"bench/#Benchmarks-1","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"bench/#Procedure-1","page":"Benchmarks","title":"Procedure","text":"","category":"section"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"These are obtained by running the bench.sh script in the test/ directory, e.g.","category":"page"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"sh bench.sh 1 2 8 16 | tee benchOutput.txt","category":"page"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"The benchmarks just involve a small selection of models, whose mutation and potential functions involve different amounts of arithmetic intensity and dimension of the particles. Clearly, it would be beneficial if the SMC community could agree on a suitable set of benchmarks to cover a wide variety of uses of SMC.","category":"page"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"The processor information is obtained using Hwloc, which may not always be reliable. For example, the X5675 has 6 physical and 12 logical cores, which differs from the output below.","category":"page"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"In the case of the E5-2667 v3 only, there is an issue with that particular machine's time stamp counter (TSC), which very adversely affects Julia's multi-threading, due to repeated system calls to obtain the time. For this reason, Julia was recompiled for this particular benchmark to disable thread profiling (this is a single preprocessor definition) and it was also run with JULIATHREADSLEEP_THRESHOLD=0, which essentially eliminates time checks to send threads to sleep.","category":"page"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"More benchmark results should eventually follow and contributions are welcome, please email them to Anthony Lee.","category":"page"},{"location":"bench/#Results-1","page":"Benchmarks","title":"Results","text":"","category":"section"},{"location":"bench/#E5-2667-v3-(Haswell)-1","page":"Benchmarks","title":"E5-2667 v3 (Haswell)","text":"","category":"section"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"x86_64-linux-gnu\nIntel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz (haswell) ; 8 Physical, 16 Logical\nJulia 0.6.2 using 1 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     427.571 μs (0 allocations: 0 bytes)\n15     14.008 ms (0 allocations: 0 bytes)\n20     475.663 ms (0 allocations: 0 bytes)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     3.062 ms (0 allocations: 0 bytes)\n15     93.969 ms (0 allocations: 0 bytes)\n20     3.076 s (0 allocations: 0 bytes)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     7.292 ms (0 allocations: 0 bytes)\n15     228.784 ms (0 allocations: 0 bytes)\n20     7.404 s (0 allocations: 0 bytes)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     18.674 ms (0 allocations: 0 bytes)\n15     677.262 ms (0 allocations: 0 bytes)\n20     20.945 s (0 allocations: 0 bytes)\n\nx86_64-linux-gnu\nIntel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz (haswell) ; 8 Physical, 16 Logical\nJulia 0.6.2 using 2 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     265.258 μs (58 allocations: 2.13 KiB)\n15     7.442 ms (58 allocations: 2.13 KiB)\n20     259.181 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     1.623 ms (58 allocations: 2.13 KiB)\n15     49.242 ms (58 allocations: 2.13 KiB)\n20     1.596 s (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     3.875 ms (70 allocations: 2.56 KiB)\n15     120.873 ms (70 allocations: 2.56 KiB)\n20     3.937 s (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     9.991 ms (58 allocations: 2.13 KiB)\n15     322.275 ms (58 allocations: 2.13 KiB)\n20     10.550 s (58 allocations: 2.13 KiB)\n\nx86_64-linux-gnu\nIntel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz (haswell) ; 8 Physical, 16 Logical\nJulia 0.6.2 using 8 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     156.166 μs (58 allocations: 2.13 KiB)\n15     3.190 ms (58 allocations: 2.13 KiB)\n20     124.468 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     499.578 μs (58 allocations: 2.13 KiB)\n15     13.897 ms (58 allocations: 2.13 KiB)\n20     461.113 ms (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     1.082 ms (70 allocations: 2.56 KiB)\n15     33.026 ms (70 allocations: 2.56 KiB)\n20     1.068 s (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     2.745 ms (58 allocations: 2.13 KiB)\n15     89.940 ms (58 allocations: 2.13 KiB)\n20     2.848 s (58 allocations: 2.13 KiB)\n\nx86_64-linux-gnu\nIntel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz (haswell) ; 8 Physical, 16 Logical\nJulia 0.6.2 using 16 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     139.753 μs (58 allocations: 2.13 KiB)\n15     1.695 ms (58 allocations: 2.13 KiB)\n20     90.893 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     486.448 μs (58 allocations: 2.13 KiB)\n15     12.934 ms (58 allocations: 2.13 KiB)\n20     422.990 ms (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     857.726 μs (70 allocations: 2.56 KiB)\n15     24.287 ms (70 allocations: 2.56 KiB)\n20     811.622 ms (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     2.205 ms (58 allocations: 2.13 KiB)\n15     68.630 ms (58 allocations: 2.13 KiB)\n20     2.269 s (58 allocations: 2.13 KiB)","category":"page"},{"location":"bench/#X5675-(Westmere)-1","page":"Benchmarks","title":"X5675 (Westmere)","text":"","category":"section"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"x86_64-pc-linux-gnu\nIntel(R) Xeon(R) CPU           X5675  @ 3.07GHz (westmere) ; 12 Physical, 12 Logical\nJulia 0.6.2 using 1 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     605.047 μs (0 allocations: 0 bytes)\n15     19.881 ms (0 allocations: 0 bytes)\n20     711.570 ms (0 allocations: 0 bytes)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     4.514 ms (0 allocations: 0 bytes)\n15     146.216 ms (0 allocations: 0 bytes)\n20     4.773 s (0 allocations: 0 bytes)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     9.210 ms (0 allocations: 0 bytes)\n15     295.984 ms (0 allocations: 0 bytes)\n20     9.642 s (0 allocations: 0 bytes)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     25.382 ms (0 allocations: 0 bytes)\n15     812.248 ms (0 allocations: 0 bytes)\n20     26.175 s (0 allocations: 0 bytes)\n\nx86_64-pc-linux-gnu\nIntel(R) Xeon(R) CPU           X5675  @ 3.07GHz (westmere) ; 12 Physical, 12 Logical\nJulia 0.6.2 using 2 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     471.944 μs (58 allocations: 2.13 KiB)\n15     10.787 ms (58 allocations: 2.13 KiB)\n20     397.859 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     2.539 ms (58 allocations: 2.13 KiB)\n15     76.381 ms (58 allocations: 2.13 KiB)\n20     2.536 s (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     4.840 ms (70 allocations: 2.56 KiB)\n15     151.000 ms (70 allocations: 2.56 KiB)\n20     4.967 s (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     13.068 ms (58 allocations: 2.13 KiB)\n15     443.880 ms (58 allocations: 2.13 KiB)\n20     13.591 s (58 allocations: 2.13 KiB)\n\nx86_64-pc-linux-gnu\nIntel(R) Xeon(R) CPU           X5675  @ 3.07GHz (westmere) ; 12 Physical, 12 Logical\nJulia 0.6.2 using 6 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     295.348 μs (58 allocations: 2.13 KiB)\n15     3.987 ms (58 allocations: 2.13 KiB)\n20     194.822 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     1.033 ms (58 allocations: 2.13 KiB)\n15     26.580 ms (58 allocations: 2.13 KiB)\n20     935.893 ms (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     1.835 ms (70 allocations: 2.56 KiB)\n15     52.459 ms (70 allocations: 2.56 KiB)\n20     1.817 s (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     4.915 ms (58 allocations: 2.13 KiB)\n15     144.660 ms (58 allocations: 2.13 KiB)\n20     4.767 s (58 allocations: 2.13 KiB)\n\nx86_64-pc-linux-gnu\nIntel(R) Xeon(R) CPU           X5675  @ 3.07GHz (westmere) ; 12 Physical, 12 Logical\nJulia 0.6.2 using 12 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     285.391 μs (58 allocations: 2.13 KiB)\n15     2.222 ms (58 allocations: 2.13 KiB)\n20     177.057 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     661.069 μs (58 allocations: 2.13 KiB)\n15     13.912 ms (58 allocations: 2.13 KiB)\n20     571.137 ms (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     1.090 ms (70 allocations: 2.56 KiB)\n15     26.662 ms (70 allocations: 2.56 KiB)\n20     1.012 s (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     2.489 ms (58 allocations: 2.13 KiB)\n15     77.098 ms (58 allocations: 2.13 KiB)\n20     2.509 s (58 allocations: 2.13 KiB)","category":"page"},{"location":"bench/#i7-4650U-(Macbook-Air-2013)-1","page":"Benchmarks","title":"i7-4650U (Macbook Air 2013)","text":"","category":"section"},{"location":"bench/#","page":"Benchmarks","title":"Benchmarks","text":"x86_64-apple-darwin14.5.0\nIntel(R) Core(TM) i7-4650U CPU @ 1.70GHz (haswell) ; 2 Physical, 4 Logical\nJulia 0.6.2 using 1 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     489.593 μs (0 allocations: 0 bytes)\n15     15.796 ms (0 allocations: 0 bytes)\n20     581.948 ms (0 allocations: 0 bytes)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     3.445 ms (0 allocations: 0 bytes)\n15     116.387 ms (0 allocations: 0 bytes)\n20     3.836 s (0 allocations: 0 bytes)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     8.501 ms (0 allocations: 0 bytes)\n15     267.428 ms (0 allocations: 0 bytes)\n20     8.842 s (0 allocations: 0 bytes)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     27.231 ms (0 allocations: 0 bytes)\n15     897.206 ms (0 allocations: 0 bytes)\n20     29.114 s (0 allocations: 0 bytes)\n\nx86_64-apple-darwin14.5.0\nIntel(R) Core(TM) i7-4650U CPU @ 1.70GHz (haswell) ; 2 Physical, 4 Logical\nJulia 0.6.2 using 2 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     341.805 μs (58 allocations: 2.13 KiB)\n15     9.639 ms (58 allocations: 2.13 KiB)\n20     383.749 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     2.100 ms (58 allocations: 2.13 KiB)\n15     71.799 ms (58 allocations: 2.13 KiB)\n20     2.187 s (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     4.759 ms (70 allocations: 2.56 KiB)\n15     153.073 ms (70 allocations: 2.56 KiB)\n20     5.038 s (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     14.911 ms (58 allocations: 2.13 KiB)\n15     487.701 ms (58 allocations: 2.13 KiB)\n20     15.623 s (58 allocations: 2.13 KiB)\n\nx86_64-apple-darwin14.5.0\nIntel(R) Core(TM) i7-4650U CPU @ 1.70GHz (haswell) ; 2 Physical, 4 Logical\nJulia 0.6.2 using 4 threads\nLinear Gaussian Model, n = 10\nlog₂N  Benchmark\n10     300.061 μs (58 allocations: 2.13 KiB)\n15     7.858 ms (58 allocations: 2.13 KiB)\n20     339.166 ms (58 allocations: 2.13 KiB)\nMultivariate Linear Gaussian Model, d = 10, n = 10\nlog₂N  Benchmark\n10     1.969 ms (58 allocations: 2.13 KiB)\n15     65.497 ms (58 allocations: 2.13 KiB)\n20     2.175 s (58 allocations: 2.13 KiB)\nSMC Sampler Example, n = 12\nlog₂N  Benchmark\n10     3.926 ms (70 allocations: 2.56 KiB)\n15     128.009 ms (70 allocations: 2.56 KiB)\n20     4.325 s (70 allocations: 2.56 KiB)\nLorenz96 Example, d = 8, n = 10\nlog₂N  Benchmark\n10     13.381 ms (58 allocations: 2.13 KiB)\n15     436.527 ms (58 allocations: 2.13 KiB)\n20     14.559 s (58 allocations: 2.13 KiB)","category":"page"},{"location":"#SequentialMonteCarlo.jl-1","page":"Contents","title":"SequentialMonteCarlo.jl","text":"","category":"section"},{"location":"#","page":"Contents","title":"Contents","text":"A light interface to serial and multi-threaded Sequential Monte Carlo","category":"page"},{"location":"#","page":"Contents","title":"Contents","text":"Pages = [ \"intro.md\", \"smcintegrals.md\", \"smcalgorithm.md\", \"smctheory.md\", \"smcve.md\", \"smcadaptive.md\", \"csmc.md\", \"impl.md\", \"performance.md\", \"smcinterface.md\", \"guide.md\", \"hmm.md\", \"bench.md\", \"refs.md\"]","category":"page"},{"location":"smcintegrals/#smcintegrals-1","page":"SMC integrals","title":"Integrals approximated by SMC","text":"","category":"section"},{"location":"smcintegrals/#The-measures-and-integrals-1","page":"SMC integrals","title":"The measures and integrals","text":"","category":"section"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"We define a sequence of measures gamma_1 ldots gamma_n by gamma_1 = M_1 and for p in 2ldotsn,","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"gamma_p(A) = int_mathsfX^p mathbf1_A(x_p) M_1(rm dx_1) prod_q=2^p G_q-1(x_q-1) M_q(x_q-1 rm dx_q) qquad A in mathcalX","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"We also define a second sequence of measures hatgamma_1ldotshatgamma_n by hatgamma_p = gamma_p cdot G_p for p in 1ldotsn.","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"Letting 1 denote the constant function x mapsto 1, the normalizing constants associated with each gamma_p and hatgamma_p are gamma_p(1) and hatgamma_p(1), respectively. We define","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"Z_p = gamma_p(1) qquad hatZ_p = hatgamma_p(1) qquad pin 1ldotsn","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"This allows us to define their normalized, probability measure counterparts","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"eta_p = gamma_p  Z_p qquad hateta_p = hatgamma_p  hatZ_p qquad p in 1ldotsn","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"We are now able to state which integrals SMC algorithms are built to approximate: these are simply gamma_p(f), hatgamma_p(f), eta_p(f) or hateta_p(f) for some p in 1ldotsn and some function fmathsfXrightarrow mathbbR.","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"We note from their definitions that Z_1 = 1 and Z_p = hatZ_p-1 for p in 2ldotsn. It follows that any of the above measures can be written in terms of the sequences hatZ_1 ldots hatZ_n, eta_1 ldots eta_n, and hateta_1 ldots hateta_n. For example, gamma_p = hatZ_p-1 eta_p for p in 2 ldots n. Therefore, it is sufficient to define SMC approximations of hatZ_p, eta_p and hateta_p for p in 1 ldots n .","category":"page"},{"location":"smcintegrals/#recursivedefmeasures-1","page":"SMC integrals","title":"Recursive definition","text":"","category":"section"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"We can alternatively define the sequence of measures mentioned above recursively by eta_1=gamma_1=M_1,","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"eta_p propto gamma_p = (gamma_p-1 cdot G_p-1) M_p qquad p in 2 ldots n","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"and for pin 1ldotsn, hateta_p propto hatgamma_p = gamma_p cdot G_p. It follows that one can write hatZ_1 = eta_1(G_1) and hatZ_p = hatZ_p-1 eta_p(G_p) for p in 2 ldots n.","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"We observe that gamma_p=hatgamma_p-1M_p, and this suggests the following diagram","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"M_1 = gamma_1 oversetG_1longrightarrow hatgamma_1 oversetM_2longrightarrow gamma_2 longrightarrow cdots longrightarrow hatgamma_p-1 oversetM_plongrightarrow gamma_p oversetG_plongrightarrow hatgamma_p longrightarrow cdots longrightarrow gamma_n oversetG_nlongrightarrow hatgamma_n","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"where measures are obtained by weighting or mutation. Alternatively, we can view the sequence of probability measures using the same diagram, i.e.","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"M_1 = eta_1 oversetG_1longrightarrow hateta_1 oversetM_2longrightarrow eta_2 longrightarrow cdots longrightarrow hateta_p-1 oversetM_plongrightarrow eta_p oversetG_plongrightarrow hateta_p longrightarrow cdots longrightarrow eta_n oversetG_nlongrightarrow hateta_n","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"but where the weighting steps also involve renormalization: eta_p oversetG_plongrightarrow hateta_p means hateta_p = (eta_p cdot G_p)  eta_p(G_p).","category":"page"},{"location":"smcintegrals/#","page":"SMC integrals","title":"SMC integrals","text":"The measures hatgamma_p and hateta_p are often referred to as the \"updated\" versions of gamma_p and eta_p, respectively.","category":"page"}]
}
